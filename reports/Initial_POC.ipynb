{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# RL For Controlled Environment Agriculture\n",
    "\n",
    "### Abstract\n",
    "---\n",
    "Make an OpenAI Gym environment for training DRL algorithms.\n",
    "Compare different algorithms to determine which are the most robust to:\n",
    "1. environmental conditions outside of training distribution\n",
    "2. transfer learning between growth environments of different physical scale"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Introduction\n",
    "---\n",
    "\n",
    "Demands on agricultural systems will increase as the world population continues to increase.\n",
    "Labor is the dominant cost component in most agricultural system.\n",
    "Unsurprisingly, most research focuses on reduction of labor costs through increased efficiency via mechanization, automation and increased yield.\n",
    "Controlled environment Agriculture (CEA) is an old idea gaining more recent momentum due to advances in automation, lighting and climate control.\n",
    "One of the fundamental challenges of CEA is optimal control of the growing environment.\n",
    "Traditional methods of optimal control in agriculture include reactive methods (thermostats) and predictive methods (PID).\n",
    "Reinforcement leaning(RL) is a well studied field that offers solutions for optimal control problems.\n",
    "The combination of RL and Deep Learning(DL) techniques created the collection of techniques known as Deep Reinforcement Learning (DRL).\n",
    "There is increasing interest in using DRL in real life optimal control problem including CEA."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Related Work\n",
    "---\n",
    "Most relevant related work is coming out of China.\n",
    "Some good papers and some very bad ones.\n",
    "All papers are essentially a proof of concept and do not rigorously explore the RL component.\n",
    "Most of the recent papers use Actor-Critic because it is the most accessible SOTA for model-free RL.\n",
    "Another common theme is using IoT devices to collect observational data.\n",
    "\n",
    "Opportunities for further research:\n",
    "- robustness to climate outside historical distribution\n",
    "- robustness to deployment in diverse cultivation structures\n",
    "- exploring value vs policy methods\n",
    "- physically informed model based RL\n",
    "\n",
    "Most relevant related work:\n",
    "- <div class=\"csl-entry\">Cao, X., Yao, Y., Li, L., Zhang, W., An, Z., Zhang, Z., Guo, S., Xiao, L., Cao, X., &#38; Luo, D. (2021). <i>IGrow: A Smart Agriculture Solution to Autonomous Greenhouse Control</i>. http://arxiv.org/abs/2107.05464</div>\n",
    "- <div class=\"csl-entry\">An, Z., Cao, X., Yao, Y., Zhang, W., Li, L., Wang, Y., Guo, S., &#38; Luo, D. (2021). <i>A Simulator-based Planning Framework for Optimizing Autonomous Greenhouse Control Strategy</i>. www.aaai.org</div>\n",
    "- <div class=\"csl-entry\">Zhang, W., Cao, X., Yao, Y., An, Z., Luo, D., &#38; Xiao, X. (2021). <i>Robust Model-based Reinforcement Learning for Autonomous Greenhouse Control</i>. http://arxiv.org/abs/2108.11645</div>\n",
    "- <div class=\"csl-entry\">Overweg, H., Berghuijs, H. N. C., &#38; Athanasiadis, I. N. (2021). <i>CropGym: a Reinforcement Learning Environment for Crop Management</i>. http://arxiv.org/abs/2104.04326</div>\n",
    "- <div class=\"csl-entry\">Wang, L., He, X., &#38; Luo, D. (2020). Deep reinforcement learning for greenhouse climate control. <i>Proceedings - 11th IEEE International Conference on Knowledge Graph, ICKG 2020</i>, 474–480. https://doi.org/10.1109/ICBK50248.2020.00073</div>\n",
    "- <div class=\"csl-entry\">Sun, L., Yang, Y., Hu, J., Porter, D., Marek, T., &#38; Hillyer, C. (2018). Reinforcement learning control for water-efficient agricultural irrigation. <i>Proceedings - 15th IEEE International Symposium on Parallel and Distributed Processing with Applications and 16th IEEE International Conference on Ubiquitous Computing and Communications, ISPA/IUCC 2017</i>, 1334–1341. https://doi.org/10.1109/ISPA/IUCC.2017.00203</div>\n",
    "- <div class=\"csl-entry\">Ban, B., &#38; Kim, S. (2017). Control of nonlinear, complex and black-boxed greenhouse system with reinforcement learning. <i>2017 International Conference on Information and Communication Technology Convergence (ICTC)</i>, 913–918. https://doi.org/10.1109/ICTC.2017.8190813</div>\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Alpha Grow\n",
    "---\n",
    "The goal is to assess which algorithms are the most robust to the following conditions:\n",
    "1. climatic conditions outside historical training distribution (generalizable)\n",
    "2. quickly adapting to new growth environments (sample efficiency)\n",
    "\n",
    "While exploring these two goals, the following will be elucidated:\n",
    "1. Are value or policy based methods best for optimal control in this domain\n",
    "2. Are model-based or model free methods more effective in this domain\n",
    "\n",
    "### 3.1 Greenhouse Gym\n",
    "To assess these research questions a simulated environment is needed.\n",
    "This simulation environment is constructed using a OpenAI Gym interface.\n",
    "OpenAI Gym interfaces are an industry standard for developing and comparing RL algorithms.\n",
    "The greenhouse simulator will calculate environmental conditions using a set of differential equations.\n",
    "Assessing value based methods will require a discrete action space and policy based methods will require a continuous action space.\n",
    "\n",
    "\n",
    "### 3.2 RL Algorithms\n",
    "RL in real world situations uses both poly-based and value-based agents as well as model and model-free algorithms.\n",
    "The most relevant recent work in DRL CEA utilized some form of Actor-Critic methodology.\n",
    "This is likely because reference implementations are readily available, and it combines policy and value based estimations.\n",
    "\n",
    "However, other challenging real-world control problems utilize probabilistic value based methods and model informed tree search.\n",
    "Therefore, I believe it is worthwhile to do a thorough investigation of what algorithms perform best in what situations.\n",
    "\n",
    "Some suitable algorithms:\n",
    "1. Deep-Q and varieties (PDQN, DoubleDQN, Rainbow)\n",
    "2. Quantile Regression DQN\n",
    "3. Vanilla policy gradient\n",
    "4. Trust region policy optimization\n",
    "5. Proximal policy optimization\n",
    "6. Deep deterministic policy gradient\n",
    "7. Twin delayed DDPG\n",
    "8. Soft Actor Critic\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Experiments\n",
    "---\n",
    "Here are some basic POC base experiments."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"Discrete Gym Environment\"\"\"\n",
    "env = gym.make(\"gym_greenhouse:greenhouse-v0\")\n",
    "print(f\"Action Space: {env.action_space}\")\n",
    "print(f\"Observation Space: {env.observation_space}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"Vanilla Deep Q\"\"\"\n",
    "from agents.DQ_Agent import DQAgent\n",
    "\n",
    "action_size = env.action_space.n\n",
    "observation_size = env.observation_space.n\n",
    "agent = DQAgent(observation_size, action_size, seed=0)\n",
    "rewards_history = []\n",
    "num_episodes = int(2e3)\n",
    "\n",
    "for i_episode in range(1, num_episodes):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "\n",
    "        action = agent.act(observation)\n",
    "        next_observation, reward, done, info = env.step(action)\n",
    "        agent.step(observation, action, reward, next_observation, done)\n",
    "        observation = next_observation\n",
    "\n",
    "    if i_episode == 1:\n",
    "        env.render()\n",
    "\n",
    "    if i_episode % 100 == 0:\n",
    "        print(f\"Episode: {i_episode}, Average Reward: {np.mean(rewards_history[-100:])}\")\n",
    "\n",
    "    rewards_history.append(np.sum(env.reward_history))\n",
    "\n",
    "print(\"Training Complete\")\n",
    "env.render()\n",
    "rewards_DQ = np.convolve(rewards_history, np.ones(10), 'valid') / 10\n",
    "x = np.arange(len(rewards_DQ))\n",
    "plt.plot(x, rewards_DQ)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Average Reward\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"Prioritized Deep Q\"\"\"\n",
    "from agents.PDQN_Agent import PDQNAgent\n",
    "\n",
    "action_size = env.action_space.n\n",
    "observation_size = env.observation_space.n\n",
    "agent = PDQNAgent(observation_size, action_size, seed=0)\n",
    "rewards_history = []\n",
    "num_episodes = int(2e3)\n",
    "\n",
    "for i_episode in range(1, num_episodes):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "\n",
    "        action = agent.act(observation)\n",
    "        next_observation, reward, done, info = env.step(action)\n",
    "        agent.step(observation, action, reward, next_observation, done)\n",
    "        observation = next_observation\n",
    "\n",
    "    if i_episode == 1:\n",
    "        env.render()\n",
    "\n",
    "    if i_episode % 100 == 0:\n",
    "        print(f\"Episode: {i_episode}, Average Reward: {np.mean(rewards_history[-100:])}\")\n",
    "\n",
    "    rewards_history.append(np.sum(env.reward_history))\n",
    "\n",
    "print(\"Training Complete\")\n",
    "env.render()\n",
    "rewards_PDQ = np.convolve(rewards_history, np.ones(10), 'valid') / 10\n",
    "x = np.arange(len(rewards_PDQ))\n",
    "plt.plot(x, rewards_PDQ)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Average Reward\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"Continuous Gym Environment\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"Deep Deterministic Policy Gradient\"\"\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Conclusion\n",
    "---\n",
    "Initial POC work is promising."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# todo\n",
    "1. create a continuous version of the greenhouse gym so that we can use policy based methods\n",
    "    - This will involve scaling the cooling to an action space of [-1, 1]\n",
    "    - handling the scaling from [-1, 1] to [min_action, max_action] in the environment makes system more robust to changing agents\n",
    "    - use box2D for the action space\n",
    "    - re-look at bipedal if confused\n",
    "2. add complexity to the physical simulation\n",
    "    - radiative heat flow\n",
    "    - air exchanges\n",
    "    - CO2\n",
    "    - humidity\n",
    "    - transpiration\n",
    "3. address todos in PDQ and DQ agent code"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}